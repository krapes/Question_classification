{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "config_fn = './config.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(config):\n",
    "    return pymysql.connect(\n",
    "        host=config['ai_db_host'],  # Database host\n",
    "        port=config['ai_db_port'],  # Database port\n",
    "        user=config['ai_db_username'],  # Database user\n",
    "        passwd=config['ai_db_password'],  # Database password\n",
    "        db=config['ai_db_name'],  # Database name\n",
    "        connect_timeout=5,\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "\n",
    "def pull_data():\n",
    "    with open(config_fn, \"r\") as f:\n",
    "        config = json.loads(f.read())\n",
    "    conn = connect(config)\n",
    "    sql_1 = \"SELECT rowId, question, category FROM cleanHotlineQuestionAnswer;\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(sql_1)\n",
    "    result = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df, df2, N, name, v=False):\n",
    "    clusterer = KMeans(n_clusters=N)\n",
    "    clusterer.fit(df2)\n",
    "    save_obj(clusterer, './models/clusterer_' + name )\n",
    "\n",
    "    transform = clusterer.transform(df2)\n",
    "    \n",
    "    d_center = []\n",
    "    cluster = []\n",
    "    for x in transform:\n",
    "        d_center.append(min(x)**2)\n",
    "        cluster.append(np.argmin(x))\n",
    "    df['cluster'] = cluster\n",
    "    df['d_from_center'] = d_center\n",
    "    d_center = np.array(d_center)\n",
    "    mean = np.mean(d_center)\n",
    "    std = np.std(d_center)\n",
    "    \n",
    "    if v == True:\n",
    "        print(\"Mean: {}\".format(round(mean, 3)))\n",
    "        print(\"STD: {}\".format(round(std, 3)))\n",
    "        print(\"\")\n",
    "        for cgroup in range(N):\n",
    "            group = df.groupby('cluster').get_group(cgroup)\n",
    "            print_clusters(group)\n",
    "\n",
    "    return df\n",
    "\n",
    "def print_clusters(group):\n",
    "    std = np.std(list(group.d_from_center))\n",
    "    print(\"Found {} messages of the same form.   STD: {}\".format(len(group), std))\n",
    "    for message in group.question.head(5):\n",
    "        if group.question.count() > 1:\n",
    "            print(message)\n",
    "            print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_tsv(df, X, cat_name):\n",
    "    vector_doc = './visualization_data/doc_vectors_' + cat_name + '.tsv'\n",
    "    count = 0\n",
    "    with open(vector_doc,'w') as w:\n",
    "        for question in X:\n",
    "            string = \"\"\n",
    "            for v in question:\n",
    "                string = string + str(v) + \"\\t\"\n",
    "            w.write(string + os.linesep)\n",
    "            count += 1\n",
    "    w.close\n",
    "    print(\"Wrote file {} with {} entries\".format(vector_doc, count))\n",
    "\n",
    "\n",
    "    meta_doc = './visualization_data/doc_meta_' + cat_name + '.tsv'\n",
    "    count = 0\n",
    "    with open(meta_doc,'w') as w:\n",
    "        w.write(\"cluster\\tquestion\\t\" + os.linesep)\n",
    "        for question, cluster in zip(list(df.question), list(df.cluster)):\n",
    "            string = \"\"\n",
    "            string = str(cluster) + \"\\t\" + str(question) + \"\\t\"\n",
    "            w.write(string + os.linesep)  \n",
    "            count += 1\n",
    "    w.close\n",
    "    print(\"Wrote file {} with {} entries\".format(meta_doc, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, N, name):\n",
    "    print(\"Loaded {} Data Points\".format(len(df)))\n",
    "    vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.7 )\n",
    "    X_vectoizer = vectorizer.fit_transform(list(df.question))\n",
    "    save_obj(vectorizer, './models/vectorizer_' + name )\n",
    "    print(\"Vectorization Complete\")\n",
    "\n",
    "    n_components = 60\n",
    "    explained_variance = 0.0\n",
    "    while explained_variance < .5 and n_components < 175:\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X_vectoizer)\n",
    "        \n",
    "        save_obj(svd, './models/svd_' + name )\n",
    "        save_obj(normalizer, './models/normalizer_' + name )\n",
    "        df[\"features\"] = list(X)\n",
    "        \n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "        n_components += 5\n",
    "    print(\"Explained variance of the SVD step: {}%     n_componets: {}\".format(\n",
    "        int(explained_variance * 100), n_components))\n",
    "    df = cluster(df, X, N, name, v=False)\n",
    "    print_to_tsv(df, X, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():    \n",
    "    df_master = pd.DataFrame(pull_data())\n",
    "    cat_names = [\"Compensation\", \"Compliance\", \"Employee Benefits\",\n",
    "                \"Leaves of Absence\", \"Recruiting and Hiring\", \"Terminations\"]\n",
    "    Ns = [20, 15, 14, 9, 9, 7]\n",
    "\n",
    "    for name, N in zip(cat_names, Ns):\n",
    "        df = df_master[df_master[\"category\"] == name].copy()\n",
    "        train_model(df, N, name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(messages, category):\n",
    "    vectorizer = load_obj( 'vectorizer_' + category )\n",
    "    svd = load_obj( 'svd_' + category )\n",
    "    normalizer = load_obj( 'normalizer_' + category )\n",
    "    clusterer = load_obj(\"clusterer_\" + category)\n",
    "\n",
    "    \n",
    "    pipeline =  make_pipeline(vectorizer, svd, normalizer)\n",
    "    messages = pipeline.transform(messages)\n",
    "\n",
    "    clusters = clusterer.predict(messages)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20275 Data Points\n",
      "Vectorization Complete\n",
      "Explained variance of the SVD step: 51%     n_componets: 110\n",
      "Wrote file ./visualization_data/doc_vectors_Compensation.tsv with 20275 entries\n",
      "Wrote file ./visualization_data/doc_meta_Compensation.tsv with 20275 entries\n",
      "Loaded 27277 Data Points\n",
      "Vectorization Complete\n",
      "Explained variance of the SVD step: 50%     n_componets: 120\n",
      "Wrote file ./visualization_data/doc_vectors_Compliance.tsv with 27277 entries\n",
      "Wrote file ./visualization_data/doc_meta_Compliance.tsv with 27277 entries\n",
      "Loaded 40236 Data Points\n",
      "Vectorization Complete\n",
      "Explained variance of the SVD step: 50%     n_componets: 130\n",
      "Wrote file ./visualization_data/doc_vectors_Employee Benefits.tsv with 40236 entries\n",
      "Wrote file ./visualization_data/doc_meta_Employee Benefits.tsv with 40236 entries\n",
      "Loaded 11513 Data Points\n",
      "Vectorization Complete\n",
      "Explained variance of the SVD step: 50%     n_componets: 115\n"
     ]
    }
   ],
   "source": [
    "train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
