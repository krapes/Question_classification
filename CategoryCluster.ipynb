{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1767,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "config_fn = './config.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(config):\n",
    "    return pymysql.connect(\n",
    "        host=config['ai_db_host'],  # Database host\n",
    "        port=config['ai_db_port'],  # Database port\n",
    "        user=config['ai_db_username'],  # Database user\n",
    "        passwd=config['ai_db_password'],  # Database password\n",
    "        db=config['ai_db_name'],  # Database name\n",
    "        connect_timeout=5,\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "\n",
    "def pull_data():\n",
    "    with open(config_fn, \"r\") as f:\n",
    "        config = json.loads(f.read())\n",
    "    conn = connect(config)\n",
    "    sql_1 = \"SELECT rowId, question, category FROM cleanHotlineQuestionAnswer;\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(sql_1)\n",
    "    result = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generlize_cluster(cluster, category, cluster_list, df):\n",
    "    \n",
    "    vectorizer, svd, normalizer, clusterer = load_model(category)\n",
    "    stop_words = list(vectorizer.stop_words_)\n",
    "   \n",
    "    word_count = Counter()\n",
    "    for message in list(df.question):\n",
    "        word_count = Counter(message.split()) + word_count\n",
    "    for word in stop_words:\n",
    "        del word_count[word]\n",
    "    string = ''\n",
    "    for word in word_count.most_common(30):\n",
    "        string += word[0] + '   '\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df, df2, N, name, v=False):\n",
    "    clusterer = KMeans(n_clusters=N)\n",
    "    clusterer.fit(list(df.features))\n",
    "    save_obj(clusterer, './models/clusterer_' + name )\n",
    "\n",
    "    transform = clusterer.transform(list(df.features))\n",
    "\n",
    "    d_center = []\n",
    "    cluster = []\n",
    "    for x in transform:\n",
    "        d_center.append(min(x)**2)\n",
    "        cluster.append(np.argmin(x))\n",
    "    df['cluster'] = cluster\n",
    "    df['d_from_center'] = d_center\n",
    "    d_center = np.array(d_center)\n",
    "    mean = np.mean(d_center)\n",
    "    std = np.std(d_center)\n",
    "    \n",
    "    if v == True:\n",
    "        print(\"Mean: {}\".format(round(mean, 3)))\n",
    "        print(\"STD: {}\".format(round(std, 3)))\n",
    "        print(\"\")\n",
    "         \n",
    "        for cgroup in range(N):\n",
    "            generlize_cluster(cgroup, name, cluster, df)\n",
    "            group = df.groupby('cluster').get_group(cgroup)\n",
    "            print_clusters(group)\n",
    "\n",
    "    return df\n",
    "\n",
    "def print_clusters(group):\n",
    "    std = np.std(list(group.d_from_center))\n",
    "    mean = np.mean(list(group.d_from_center))\n",
    "    \n",
    "    center = group[group[\"d_from_center\"] == min(group[\"d_from_center\"])]\n",
    "\n",
    "    center.drop_duplicates(subset=['question'], inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Found {} messages of the same form.  Mean: {} STD: {}\".format(len(group), mean, std))\n",
    "    print(\"*** {} ***\".format(list(center.question)[0]))\n",
    "    print(\"\")\n",
    "    for message in group.question.head(5):\n",
    "        if group.question.count() > 1:\n",
    "            print(message)\n",
    "            print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1772,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_tsv(df, X, cat_name):\n",
    "    vector_doc = './visualization_data/doc_vectors_' + cat_name + '.tsv'\n",
    "    count = 0\n",
    "    with open(vector_doc,'w') as w:\n",
    "        for question in X:\n",
    "            string = \"\"\n",
    "            for v in question:\n",
    "                string = string + str(v) + \"\\t\"\n",
    "            w.write(string + os.linesep)\n",
    "            count += 1\n",
    "    w.close\n",
    "    print(\"Wrote file {} with {} entries\".format(vector_doc, count))\n",
    "\n",
    "\n",
    "    meta_doc = './visualization_data/doc_meta_' + cat_name + '.tsv'\n",
    "    count = 0\n",
    "    with open(meta_doc,'w') as w:\n",
    "        w.write(\"cluster\\tquestion\\t\" + os.linesep)\n",
    "        for question, cluster in zip(list(df.question), list(df.cluster)):\n",
    "            string = \"\"\n",
    "            string = str(cluster) + \"\\t\" + str(question) + \"\\t\"\n",
    "            w.write(string + os.linesep)  \n",
    "            count += 1\n",
    "    w.close\n",
    "    print(\"Wrote file {} with {} entries\".format(meta_doc, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, N, name):\n",
    "    print(\"Loaded {} Data Points\".format(len(df)))\n",
    "    vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.7 )\n",
    "    X_vectoizer = vectorizer.fit_transform(list(df.question))\n",
    "    save_obj(vectorizer, './models/vectorizer_' + name )\n",
    "    print(\"Vectorization Complete\")\n",
    "\n",
    "    n_components = 60\n",
    "    explained_variance = 0.0\n",
    "    while explained_variance < .5 and n_components < 175:\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X_vectoizer)\n",
    "        \n",
    "        save_obj(svd, './models/svd_' + name )\n",
    "        save_obj(normalizer, './models/normalizer_' + name )\n",
    "        df[\"features\"] = list(X)\n",
    "        \n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "        n_components += 5\n",
    "    print(\"Explained variance of the SVD step: {}%     n_componets: {}\".format(\n",
    "        int(explained_variance * 100), n_components))\n",
    "    df = cluster(df, X, N, name, v=True)\n",
    "    print_to_tsv(df, X, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():    \n",
    "    df_master = pd.DataFrame(pull_data())\n",
    "    cat_names = [\"Terminations\"]\n",
    "    #cat_names = [\"Compensation\"#, \"Compliance\", \"Employee Benefits\",\n",
    "                #\"Leaves of Absence\", \"Recruiting and Hiring\", \"Terminations\"]\n",
    "    Ns = [20, 15, 14, 9, 9, 7]\n",
    "    Ns = [7]\n",
    "\n",
    "    for name, N in zip(cat_names, Ns):\n",
    "        df = df_master[df_master[\"category\"] == name].copy()[:100]\n",
    "        train_model(df, N, name)\n",
    "        df[\"prediction\"] = predict(list(df.question), name)\n",
    "        correct = len(df[df[\"cluster\"] == df[\"prediction\"]])\n",
    "        total = float(len(df))\n",
    "        print(\"The model {} consistantly classified {}% of the dataset\".format(name, round(correct/total * 100, 1)))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(messages, category):\n",
    "    \"This predicts the cluster of a message based on the kmeans model for the given category\"\n",
    "    \"Inputs:\"\n",
    "    \"        messages: a list of strings to be classified\"\n",
    "    \"        category: a string containing the model to be used\"\n",
    "    \"Output:\"\n",
    "    \"        clusters: a list of intergers corresponding to the message classification cluster\"\n",
    "    \n",
    "    vectorizer, svd, normalizer, clusterer = load_model(category)\n",
    "\n",
    "    \n",
    "    pipeline =  make_pipeline(vectorizer, svd, normalizer)\n",
    "    messages = pipeline.transform(messages)\n",
    "\n",
    "    clusters = clusterer.predict(messages)\n",
    "    return clusters\n",
    "\n",
    "def load_model(category):\n",
    "    vectorizer = load_obj( './models/vectorizer_' + category )\n",
    "    svd = load_obj( './models/svd_' + category )\n",
    "    normalizer = load_obj( './models/normalizer_' + category )\n",
    "    clusterer = load_obj(\"./models/clusterer_\" + category)\n",
    "    return vectorizer, svd, normalizer, clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
