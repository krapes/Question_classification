{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages, word_drop=True):\n",
    "    cvocabularies = Counter()\n",
    "    #print(\"building vocabulary from {} messages\".format(len(messages)))\n",
    "    for start in range(0, len(messages), 500):\n",
    "        vocabulary = Counter()\n",
    "        for message in messages[start:start + 500]:\n",
    "            message = str(message)\n",
    "            #message = remove_nonalphanumeric(message)\n",
    "            message_split = message.split()\n",
    "            gram_count = 3\n",
    "            grams = []\n",
    "            for i in range(len(message_split) - (gram_count - 1)):\n",
    "                gram = ''\n",
    "                for n in range(gram_count):\n",
    "                    gram = gram + message_split[i+n] + ' '\n",
    "                grams.append(gram[:-1])\n",
    "            vocabulary = vocabulary + Counter(message_split) + Counter(grams)\n",
    "        cvocabularies = cvocabularies + vocabulary\n",
    "    if word_drop == True:\n",
    "        for key, count in dropwhile(\n",
    "                                    lambda key_count: key_count[1] >= (len(messages) * .01),\n",
    "                                    cvocabularies.most_common()):\n",
    "            del cvocabularies[key]\n",
    "    #print(\"The vocabulary contains {} words\".format(len(cvocabularies)))\n",
    "    return cvocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list(file):\n",
    "    vocab = []\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            else:\n",
    "                print(\"Repeat: {}\".format(word))\n",
    "    return vocab\n",
    "\n",
    "def label_features(df, features_master):\n",
    "        for i,row in df.iterrows():\n",
    "            #message = pruning_dict.remove_nonalphanumeric(row.text)\n",
    "            message = str(row.question_class)\n",
    "            features = build_vocabulary([message], word_drop=False) & features_master\n",
    "            features = features + features_master\n",
    "            features = list(np.array(list(features.values())) - 1)\n",
    "            df.set_value(i,'features',features)\n",
    "        return df\n",
    "\n",
    "def create_feature_dataframe(df, features_master):\n",
    "    return pd.DataFrame(list(df.features), columns=range(len(features_master)))\n",
    "\n",
    "def cluster_filter2(df, df2, N, features_master, v=False):\n",
    "    #print(\"Start loop\")\n",
    "    gpercentages = []\n",
    "    singles = 0\n",
    "    clusterer = KMeans(n_clusters=N)\n",
    "    clusterer.fit(df2)\n",
    "    transform = clusterer.transform(df2)\n",
    "    d_center = []\n",
    "    cluster = []\n",
    "    for x in transform:\n",
    "        d_center.append(min(x)**2)\n",
    "        cluster.append(np.argmin(x))\n",
    "    df['cluster'] = cluster\n",
    "    df['d_from_center'] = d_center\n",
    "    d_center = np.array(d_center)\n",
    "    mean = np.mean(d_center)\n",
    "    std = np.std(d_center)\n",
    "    '''\n",
    "    for cgroup in range(N):\n",
    "        group = df.groupby('cluster').get_group(cgroup)\n",
    "        if group.question_class.count() <= len(df) * .01:\n",
    "            df = df.drop(group.index)\n",
    "            singles += 1\n",
    "    print(\"# of singles: {}\".format(singles))\n",
    "    print(\"df length: {}\".format(len(df)))\n",
    "    if singles >= 6 and N <= (len(df) - singles):\n",
    "        df = cluster_filter2(df, create_feature_dataframe(df, features_master), N, features_master)\n",
    "        return df\n",
    "    '''\n",
    "    for cgroup in range(N):\n",
    "        group = df.groupby('cluster').get_group(cgroup)\n",
    "        gpercent = (compute_gpercentage(group, mean, std))\n",
    "        gpercentages.append(gpercent)\n",
    "        '''\n",
    "        if v == True:\n",
    "            print(\"Found {} messages of the same form with a gpercent of {}.\".format(len(group), gpercent))\n",
    "            for message in group.question_class.head(3):\n",
    "                if group.question_class.count() > 1:\n",
    "                    print(message)\n",
    "                    print(\"\")\n",
    "            print(\"\")\n",
    "        '''\n",
    "            \n",
    "    \n",
    "    median = np.median(np.array(gpercentages))\n",
    "    if v == True:\n",
    "        for cgroup in range(N):\n",
    "            group = df.groupby('cluster').get_group(cgroup)\n",
    "            gpercent = (compute_gpercentage(group, mean, std))\n",
    "            if gpercent >= median:\n",
    "                print_clusters(group, gpercent)\n",
    "    else:\n",
    "        pass\n",
    "        #print(median)\n",
    "        #print(np.mean(np.array(gpercentages)))\n",
    "    return df, median\n",
    "\n",
    "def compute_gpercentage(group, mean, std):\n",
    "\n",
    "    gscore = 0.0\n",
    "    for i, row in group.iterrows():\n",
    "        z = (row.d_from_center - mean) / std\n",
    "        if z < -0.68:\n",
    "            gscore += 1\n",
    "    glength = len(group)\n",
    "    gpercent = gscore/glength\n",
    "    return gpercent\n",
    "\n",
    "def print_clusters(group, gpercent):\n",
    "    print(\"Found {} messages of the same form with a gpercent of {}.\".format(len(group), gpercent))\n",
    "    for message in group.question_class.head(5):\n",
    "        if group.question_class.count() > 1:\n",
    "            print(message)\n",
    "            print(\"\")\n",
    "    print(\"\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 2\n",
      "N: 3\n",
      "The best clustering is 2:\n",
      "Found 12288 messages of the same form with a gpercent of 0.218994140625.\n",
      "question use intellectual property employment ends please give call\n",
      "\n",
      "several questions following topics\n",
      "\n",
      "recently added physician payroll hired compensation rate paid per day day working short time period two weeks decided classify since performing main operations business question even though working weeks classify salary exempt employee two weeks main reason need working full hour day without set lunch time breaks\n",
      "\n",
      "employee hired several weeks ago never provided documents needed verification week told us could provide needed documents proceed\n",
      "\n",
      "eeo report due new company appears last completed eeo report september data research internet states report due correct\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvdata = \"./data/subcategory_2_think_hr_v2.csv\"\n",
    "df = pd.read_csv(csvdata)\n",
    "df = df[['question_class']]\n",
    "\n",
    "#vocab = feature_list('./HotWords.csv')\n",
    "vocab = load_obj('final_feature_set')\n",
    "\n",
    "features_master = Counter(vocab)\n",
    "df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "df2 = create_feature_dataframe(df, features_master)\n",
    "\n",
    "medians = []\n",
    "for N in range(2,4):\n",
    "    print(\"N: {}\".format(N))\n",
    "    df, median = cluster_filter2(df, df2, N, features_master)\n",
    "    medians.append([median, N])\n",
    "\n",
    "best_clustering_index = np.argmax(medians, axis=0)\n",
    "best_clustering = medians[best_clustering_index[0]][1]\n",
    "print(\"The best clustering is {}:\".format(best_clustering))\n",
    "df, median = cluster_filter2(df, df2, best_clustering, features_master, v=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
