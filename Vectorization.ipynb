{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name _utils",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-48a9e2c5f60b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdropwhile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalinski_harabaz_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\sklearn\\cluster\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspectral\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspectral_clustering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m from .mean_shift_ import (mean_shift, MeanShift,\n\u001b[0;32m      8\u001b[0m                           estimate_bandwidth, get_bin_seeds)\n",
      "\u001b[1;32mC:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\sklearn\\cluster\\spectral.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mk_means_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mk_means\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\sklearn\\manifold\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmds\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmacof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspectral_embedding_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpectralEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mt_sne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m __all__ = ['locally_linear_embedding', 'LocallyLinearEmbedding', 'Isomap',\n",
      "\u001b[1;32mC:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\sklearn\\manifold\\t_sne.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_barnes_hut_tsne\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name _utils"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages, word_drop=True):\n",
    "    cvocabularies = Counter()\n",
    "    #print(\"building vocabulary from {} messages\".format(len(messages)))\n",
    "    for start in range(0, len(messages), 500):\n",
    "        vocabulary = Counter()\n",
    "        for message in messages[start:start + 500]:\n",
    "            message = str(message)\n",
    "            #message = remove_nonalphanumeric(message)\n",
    "            message_split = message.split()\n",
    "            gram_count = 3\n",
    "            grams = []\n",
    "            for i in range(len(message_split) - (gram_count - 1)):\n",
    "                gram = ''\n",
    "                for n in range(gram_count):\n",
    "                    gram = gram + message_split[i+n] + ' '\n",
    "                grams.append(gram[:-1])\n",
    "            vocabulary = vocabulary + Counter(message_split) + Counter(grams)\n",
    "        cvocabularies = cvocabularies + vocabulary\n",
    "    if word_drop == True:\n",
    "        for key, count in dropwhile(\n",
    "                                    lambda key_count: key_count[1] >= (len(messages) * .01),\n",
    "                                    cvocabularies.most_common()):\n",
    "            del cvocabularies[key]\n",
    "    #print(\"The vocabulary contains {} words\".format(len(cvocabularies)))\n",
    "    return cvocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list(file):\n",
    "    vocab = []\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            else:\n",
    "                print(\"Repeat: {}\".format(word))\n",
    "    return vocab\n",
    "\n",
    "def label_features(df, features_master):\n",
    "        for i,row in df.iterrows():\n",
    "            #message = pruning_dict.remove_nonalphanumeric(row.text)\n",
    "            message = str(row.question_class)\n",
    "            features = build_vocabulary([message], word_drop=False) & features_master\n",
    "            features = features + features_master\n",
    "            features = list(np.array(list(features.values())) - 1)\n",
    "            df.set_value(i,'features',features)\n",
    "        return df\n",
    "\n",
    "def create_feature_dataframe(df, features_master):\n",
    "    return pd.DataFrame(list(df.features), columns=range(len(features_master)))\n",
    "\n",
    "def cluster_filter2(df, df2, N, features_master, v=False):\n",
    "    #print(\"Start loop\")\n",
    "    #gpercentages = []\n",
    "    #singles = 0\n",
    "    clusterer = KMeans(n_clusters=N)\n",
    "    clusterer.fit(df2)\n",
    "    transform = clusterer.transform(df2)\n",
    "    \n",
    "    \n",
    "    d_center = []\n",
    "    cluster = []\n",
    "    for x in transform:\n",
    "        d_center.append(min(x)**2)\n",
    "        cluster.append(np.argmin(x))\n",
    "    df['cluster'] = cluster\n",
    "    df['d_from_center'] = d_center\n",
    "    d_center = np.array(d_center)\n",
    "    mean = np.mean(d_center)\n",
    "    std = np.std(d_center)\n",
    "    silhouette_avg = silhouette_score(df2, df.cluster)\n",
    "    calinski = calinski_harabaz_score(df2, df.cluster)\n",
    "    print(\"Silhouette: {}\".format(round(silhouette_avg,3)))\n",
    "    print(\"Calinski_Harabaz: {}\".format(round(calinski, 3)))\n",
    "    print(\"Mean: {}\".format(round(mean, 3)))\n",
    "    print(\"STD: {}\".format(round(std, 3)))\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    '''\n",
    "    for cgroup in range(N):\n",
    "        group = df.groupby('cluster').get_group(cgroup)\n",
    "        if group.question_class.count() <= len(df) * .01:\n",
    "            df = df.drop(group.index)\n",
    "            singles += 1\n",
    "    print(\"# of singles: {}\".format(singles))\n",
    "    print(\"df length: {}\".format(len(df)))\n",
    "    if singles >= 6 and N <= (len(df) - singles):\n",
    "        df = cluster_filter2(df, create_feature_dataframe(df, features_master), N, features_master)\n",
    "        return df\n",
    "\n",
    "    for cgroup in range(N):\n",
    "        group = df.groupby('cluster').get_group(cgroup)\n",
    "        gpercent = (compute_gpercentage(group, mean, std))\n",
    "        gpercentages.append(gpercent)\n",
    "\n",
    "\n",
    "        if v == True:\n",
    "            print(\"Found {} messages of the same form with a gpercent of {}.\".format(len(group), gpercent))\n",
    "            for message in group.question_class.head(3):\n",
    "                if group.question_class.count() > 1:\n",
    "                    print(message)\n",
    "                    print(\"\")\n",
    "\n",
    "            \n",
    "    \n",
    "    #median = np.median(np.array(gpercentages))\n",
    "    '''\n",
    "    if v == True:\n",
    "        for cgroup in range(N):\n",
    "            group = df.groupby('cluster').get_group(cgroup)\n",
    "            print_clusters(group)\n",
    "    else:\n",
    "        pass\n",
    "        #print(median)\n",
    "        #print(np.mean(np.array(gpercentages)))\n",
    "    \n",
    "    return df, silhouette_avg\n",
    "\n",
    "def compute_gpercentage(group, mean, std):\n",
    "\n",
    "    gscore = 0.0\n",
    "    for i, row in group.iterrows():\n",
    "        z = (row.d_from_center - mean) / std\n",
    "        if z < -0.68:\n",
    "            gscore += 1\n",
    "    glength = len(group)\n",
    "    gpercent = gscore/glength\n",
    "    return gpercent\n",
    "\n",
    "def print_clusters(group):\n",
    "    print(\"Found {} messages of the same form.\".format(len(group)))\n",
    "    for message in group.question_class.head(5):\n",
    "        if group.question_class.count() > 1:\n",
    "            print(message)\n",
    "            print(\"\")\n",
    "    print(\"\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdata = \"./data/subcategory_2_think_hr_v2.csv\"\n",
    "df = pd.read_csv(csvdata)\n",
    "df = df[['question_class']][:100]\n",
    "\n",
    "'''\n",
    "#vocab = feature_list('./HotWords.csv')\n",
    "vocab = load_obj('final_feature_set')\n",
    "\n",
    "features_master = Counter(vocab)\n",
    "df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "df2 = create_feature_dataframe(df, features_master)\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=50, random_state=42)\n",
    "df2 = svd.fit_transform(df2)\n",
    "\n",
    "\n",
    "medians = []\n",
    "for N in range(10,35):\n",
    "    print(\"N: {}\".format(N))\n",
    "    df, median = cluster_filter2(df, df2, N, features_master)\n",
    "    medians.append([median, N])\n",
    "\n",
    "best_clustering_index = np.argmax(medians, axis=0)\n",
    "best_clustering = medians[best_clustering_index[0]][1]\n",
    "print(\"The best clustering is {}:\".format(best_clustering))\n",
    "df, median = cluster_filter2(df, df2, best_clustering, features_master, v=True)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=2, random_state=42)\n",
    "df[\"plot_cord\"] = list(svd.fit_transform(df2))\n",
    "\n",
    "#df2 = create_feature_dataframe(df, features_master)\n",
    "transformed = pd.DataFrame(svd.fit_transform(df2), columns=[\"plot_cordX\", \"plot_cordY\"])\n",
    "meanX = transformed[\"plot_cordX\"].mean()\n",
    "meanY = transformed[\"plot_cordY\"].mean()\n",
    "transformed[\"plot_cordX\"] = transformed[\"plot_cordX\"].divide(meanX) - 1\n",
    "transformed[\"plot_cordY\"] = transformed[\"plot_cordY\"].divide(meanY) - 1\n",
    "meanX = transformed[\"plot_cordX\"].mean()\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(df)\n",
    "\n",
    "svd = TruncatedSVD(n_components=7)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print(X)\n",
    "\n",
    "df[\"plot_cordX\"] = transformed.plot_cordX\n",
    "df[\"plot_cordY\"] = transformed.plot_cordY\n",
    "\n",
    "q = df[\"plot_cordX\"].quantile(0.95)\n",
    "df = df[df[\"plot_cordX\"] < q]\n",
    "q = df[\"plot_cordY\"].quantile(0.95)\n",
    "df = df[df[\"plot_cordY\"] < q]\n",
    "\n",
    "\n",
    "for n in range(best_clustering):\n",
    "    plt.scatter(df[df[\"cluster\"] == n].plot_cordX, df[df[\"cluster\"] == n].plot_cordY, label=\"Class \" + str(n))\n",
    "\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
