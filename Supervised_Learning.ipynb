{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list(file):\n",
    "    vocab = []\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            else:\n",
    "                print(\"Repeat: {}\".format(word))\n",
    "    return vocab\n",
    "\n",
    "def label_features(df, features_master):\n",
    "        for i,row in df.iterrows():\n",
    "            #message = pruning_dict.remove_nonalphanumeric(row.text)\n",
    "            message = str(row.question_class)\n",
    "            features = Counter(message.split()) & features_master\n",
    "            features = features + features_master\n",
    "            features = list(np.array(list(features.values())) - 1)\n",
    "            df.set_value(i,'features',features)\n",
    "        return df\n",
    "\n",
    "def create_feature_dataframe(df, features_master):\n",
    "    return pd.DataFrame(list(df.features), columns=range(len(features_master)))\n",
    "\n",
    "def split_set(x, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(x),\n",
    "                                                        list(y),\n",
    "                                                        test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages, word_drop=True):\n",
    "        vocabulary = Counter()\n",
    "        print(\"building vocabulary from {} messages\".format(len(messages)))\n",
    "        for message in messages:\n",
    "            message = str(message)\n",
    "            #message = remove_nonalphanumeric(message)\n",
    "            message_split = message.split()\n",
    "            gram_count = 2\n",
    "            grams = []\n",
    "            for i in range(len(message_split) - 1):\n",
    "                gram = ''\n",
    "                for n in range(gram_count):\n",
    "                    gram = gram + message_split[i+n] + ' '\n",
    "                grams.append(gram[:-1])\n",
    "            vocabulary = vocabulary + Counter(message_split) + Counter(grams)\n",
    "        if word_drop == True:\n",
    "            for key, count in dropwhile(\n",
    "                                        lambda key_count: key_count[1] >= (len(messages) * .01),\n",
    "                                        vocabulary.most_common()):\n",
    "                del vocabulary[key]\n",
    "        print(\"The vocabulary contains {} words\".format(len(vocabulary)))\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocab(df, vocabulary, group1, group2, percent_saved):\n",
    "\n",
    "    info = df.groupby('category').get_group(group1)\n",
    "    info_v = build_vocabulary(info.question_class, word_drop=True)\n",
    "    info_w = info_v.keys()\n",
    "    express = df.groupby('category').get_group(group2)\n",
    "    express_v = build_vocabulary(express.question_class, word_drop=True)\n",
    "    express_w = express_v.keys()\n",
    "\n",
    "    common_v = info_v & express_v\n",
    "    common_w = common_v.keys()\n",
    "\n",
    "    words = []\n",
    "    ratios = []\n",
    "\n",
    "    for word in info_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(info_v[word])\n",
    "            words.append(word)\n",
    "\n",
    "    for word in common_w:\n",
    "        ratios.append(info_v[word] / express_v[word])\n",
    "        words.append(word)\n",
    "\n",
    "    for word in express_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(express_v[word] * -1)\n",
    "            words.append(word)\n",
    "\n",
    "\n",
    "    threshold = int(len(words) * (percent_saved/2))\n",
    "    top20 = np.argsort(ratios)[-threshold:]\n",
    "    bottom20 = np.argsort(ratios)[:threshold]\n",
    "\n",
    "    polar_words = []\n",
    "    for group in [top20, bottom20]:\n",
    "        for index in group:\n",
    "            polar_words.append(words[index])\n",
    "\n",
    "\n",
    "    vocabulary = Counter(polar_words) & vocabulary\n",
    "\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "building vocabulary from 500 messages\n",
      "The vocabulary contains 741 words\n",
      "building vocabulary from 161 messages\n",
      "The vocabulary contains 865 words\n",
      "building vocabulary from 190 messages\n",
      "The vocabulary contains 1149 words\n",
      "The length of the pruned vocab between Compensation and Compliance is 520\n",
      "building vocabulary from 161 messages\n",
      "The vocabulary contains 865 words\n",
      "building vocabulary from 149 messages\n",
      "The vocabulary contains 1220 words\n",
      "The length of the pruned vocab between Compensation and Employee Benefits is 590\n",
      "building vocabulary from 190 messages\n",
      "The vocabulary contains 1149 words\n",
      "building vocabulary from 149 messages\n",
      "The vocabulary contains 1220 words\n",
      "The length of the pruned vocab between Compliance and Employee Benefits is 493\n",
      "Length of final vocab 201\n",
      "['money', 'executive', 'work week', 'years', 'new hire', 'area', 'find', 'staff', 'non exempt', 'member', 'believe', 'much', 'outside', 'better', 'th', 'exempt', 'employed', 'main', 'employees work', 'resources', 'hours worked', 'count', 'vendor', 'closed', 'stay', 'terminate', 'breaks', 'practice', 'schedule', 'accrue', 'break', 'lunch', 'desk', 'companies', 'applicable', 'taxes', 'requesting', 'small', 'obligated', 'www', 'set', 'attachments', 'fair', 'people', 'rate', 'used', 'current plan', 'ees', 'exempt employees', 'close', 'calling', 'happens', 'hourly', 'part time', 'federal', 'section', 'missing', 'anniversary', 'per', 'access', 'enough', 'goes', 'toll', 'worked', 'email', 'wage', 'child support', 'city', 'one employee', 'payroll', 'understanding', 'hours', 'clarify', 'qualify', 'test', 'pay employee', 'ask', 'employees working', 'last', 'license', 'getting', 'receive', 'shift', 'taking', 'compliance', 'job', 'drug', 'day holiday', 'deduction', 'unpaid', 'amount', 'social', 'lunch break', 'effective date', 'changes', 'paid sick', 'comes', 'private', 'three', 'county']\n",
      "Training NerualNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NerualNet is 0.6\n",
      "Training DecisionTree\n",
      "The accuracy for DecisionTree is 0.66\n",
      "Training AdaBoost\n",
      "The accuracy for AdaBoost is 0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "csvdata = \"./data/subcategory_2_think_hr_v2.csv\"\n",
    "df = pd.read_csv(csvdata)\n",
    "df = df[:500]\n",
    "catoregories = [\"Compensation\", \"Compliance\", \"Employee Benefits\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(catoregories)\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit([[0],[1],[2]])\n",
    "\n",
    "\n",
    "ohe_array = ohe.transform(le.transform(df.category).reshape(-1,1)).toarray()\n",
    "df['category_ohe'] = list(ohe_array)\n",
    "\n",
    "\n",
    "#vocab = feature_list('./HotWords.csv')\n",
    "vocab = build_vocabulary(df.question_class)\n",
    "pairs = [[0,1], [0,2], [1,2]]\n",
    "vocabularies = []\n",
    "for pair in pairs:\n",
    "    a,b = pair\n",
    "    group1 = catoregories[a]\n",
    "    group2 = catoregories[b]\n",
    "    vocab_CC = prune_vocab(df, vocab, group1, group2, .9)\n",
    "    vocabularies.append(vocab_CC)\n",
    "    print(\"The length of the pruned vocab between {} and {} is {}\".format(group1, group2, len(vocab_CC)))\n",
    "\n",
    "vocab = vocabularies[0] & vocabularies[1] & vocabularies[2]\n",
    "print(\"Length of final vocab {}\".format(len(vocab)))\n",
    "vocab = list(vocab)\n",
    "print(vocab[:100])\n",
    "\n",
    "features_master = Counter(vocab)\n",
    "df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "df2 = create_feature_dataframe(df, features_master)\n",
    "X_train, X_test, y_train, y_test = split_set(df.features, df.category, 0.1)\n",
    "\n",
    "names = [\"NerualNet\", \"DecisionTree\", \"AdaBoost\"]\n",
    "clfs = [MLPClassifier( max_iter=500),\n",
    "            DecisionTreeClassifier(),\n",
    "            AdaBoostClassifier()]\n",
    "for clf, name in zip(clfs, names):\n",
    "    print(\"Training {}\".format(name))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    #print(y_predict)\n",
    "    score = accuracy_score(y_test, y_predict)\n",
    "    print(\"The accuracy for {} is {}\".format(name, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
