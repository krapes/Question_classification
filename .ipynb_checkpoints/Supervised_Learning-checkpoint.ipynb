{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "import pickle\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list(file):\n",
    "    vocab = []\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            else:\n",
    "                print(\"Repeat: {}\".format(word))\n",
    "    return vocab\n",
    "\n",
    "def label_features(df, features_master):\n",
    "        for i,row in df.iterrows():\n",
    "            #message = pruning_dict.remove_nonalphanumeric(row.text)\n",
    "            message = str(row.question_class)\n",
    "            features = Counter(message.split()) & features_master\n",
    "            features = features + features_master\n",
    "            features = list(np.array(list(features.values())) - 1)\n",
    "            df.set_value(i,'features',features)\n",
    "        return df\n",
    "\n",
    "def create_feature_dataframe(df, features_master):\n",
    "    return pd.DataFrame(list(df.features), columns=range(len(features_master)))\n",
    "\n",
    "def split_set(x, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(x),\n",
    "                                                        list(y),\n",
    "                                                        test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages, word_drop=True):\n",
    "    cvocabularies = Counter()\n",
    "    print(\"building vocabulary from {} messages\".format(len(messages)))\n",
    "    for start in range(0, len(messages), 500):\n",
    "        vocabulary = Counter()\n",
    "        for message in messages[start:start + 500]:\n",
    "            message = str(message)\n",
    "            #message = remove_nonalphanumeric(message)\n",
    "            message_split = message.split()\n",
    "            gram_count = 3\n",
    "            grams = []\n",
    "            for i in range(len(message_split) - (gram_count - 1)):\n",
    "                gram = ''\n",
    "                for n in range(gram_count):\n",
    "                    gram = gram + message_split[i+n] + ' '\n",
    "                grams.append(gram[:-1])\n",
    "            vocabulary = vocabulary + Counter(message_split) + Counter(grams)\n",
    "        cvocabularies = cvocabularies + vocabulary\n",
    "    if word_drop == True:\n",
    "        for key, count in dropwhile(\n",
    "                                    lambda key_count: key_count[1] >= (len(messages) * .01),\n",
    "                                    cvocabularies.most_common()):\n",
    "            del cvocabularies[key]\n",
    "    print(\"The vocabulary contains {} words\".format(len(cvocabularies)))\n",
    "    return cvocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocab(df, vocabulary, group1, group2, percent_saved):\n",
    "\n",
    "    try:\n",
    "        info_v = load_obj(group1)\n",
    "    except:\n",
    "        info = df.groupby('category').get_group(group1)\n",
    "        info_v = build_vocabulary(info.question_class, word_drop=True)\n",
    "        save_obj(info_v, group1)\n",
    "    info_w = info_v.keys()\n",
    "    try:\n",
    "        express_v = load_obj(group2)\n",
    "    except:\n",
    "        express = df.groupby('category').get_group(group2)\n",
    "        express_v = build_vocabulary(express.question_class, word_drop=True)\n",
    "        save_obj(express_v, group2)\n",
    "    express_w = express_v.keys()\n",
    "\n",
    "    common_v = info_v & express_v\n",
    "    common_w = common_v.keys()\n",
    "\n",
    "    words = []\n",
    "    ratios = []\n",
    "\n",
    "    for word in info_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(info_v[word]/float(len(info_v)))\n",
    "            words.append(word)\n",
    "\n",
    "    for word in common_w:\n",
    "        ratios.append((info_v[word]/float(len(info_v))) / (express_v[word]/float(len(express_v))))\n",
    "        words.append(word)\n",
    "\n",
    "    for word in express_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(express_v[word]/float(len(express_v) * -1))\n",
    "            words.append(word)\n",
    "\n",
    "\n",
    "    threshold = int(len(words) * (percent_saved/2))\n",
    "    top20 = np.argsort(ratios)[-threshold:]\n",
    "    bottom20 = np.argsort(ratios)[:threshold]\n",
    "\n",
    "    polar_words = []\n",
    "    for group in [top20, bottom20]:\n",
    "        for index in group:\n",
    "            polar_words.append(words[index])\n",
    "\n",
    "    vocabulary = Counter(polar_words) & vocabulary\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def polarize_dict(df, vocab_m, catoregories, keep_percent):\n",
    "    pairs = [[0,1], [0,2], [1,2]]\n",
    "    vocabularies = []\n",
    "    for pair in pairs:\n",
    "        a,b = pair\n",
    "        group1 = catoregories[a]\n",
    "        group2 = catoregories[b]\n",
    "        vocab_CC = prune_vocab(df, vocab_m, group1, group2, keep_percent/10.0)\n",
    "        vocabularies.append(vocab_CC)\n",
    "        #print(\"The length of the pruned vocab between {} and {} is {}\".format(group1, group2, len(vocab_CC)))\n",
    "\n",
    "    vocab = vocabularies[0] & vocabularies[1] & vocabularies[2]\n",
    "    print(\"Length of final vocab {}\".format(len(vocab)))\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, v=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_set(df.features, df.category, 0.2)\n",
    "\n",
    "    names = [\"NerualNet\", \"DecisionTree\", \"AdaBoost\"]\n",
    "    clfs = [MLPClassifier( max_iter=500),\n",
    "                DecisionTreeClassifier()]#,\n",
    "                #AdaBoostClassifier()]\n",
    "    accuracies = []\n",
    "    for clf, name in zip(clfs, names):\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        '''\n",
    "        print(y_test)\n",
    "        print(\"y_predict\")\n",
    "        print(y_predict)\n",
    "        '''\n",
    "        y_test2 = np.asarray(y_test)\n",
    "        y_test2 = y_test2.dot(ohe.active_features_).astype(int)\n",
    "        y_predict = np.asarray(y_predict.dot(ohe.active_features_).astype(int))\n",
    "        #print(y_test2, y_predict)\n",
    "        score = accuracy_score(y_test2, y_predict)\n",
    "        if v == True:\n",
    "            print(\"The accuracy for {} is {}\".format(name, score))\n",
    "        accuracies.append(score)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Training with 50%\n",
      "Length of final vocab 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy was [0.25]\n",
      "Training with 60%\n",
      "Length of final vocab 128\n",
      "The mean accuracy was [0.25, 0.5]\n",
      "Training with 70%\n",
      "Length of final vocab 192\n",
      "The mean accuracy was [0.25, 0.5, 0.5833333333333334]\n",
      "Training with 80%\n",
      "Length of final vocab 270\n",
      "The mean accuracy was [0.25, 0.5, 0.5833333333333334, 0.3333333333333333]\n",
      "Training with 90%\n",
      "Length of final vocab 360\n",
      "The mean accuracy was [0.25, 0.5, 0.5833333333333334, 0.3333333333333333, 0.25]\n",
      "Training with 100%\n",
      "Length of final vocab 463\n",
      "The mean accuracy was [0.25, 0.5, 0.5833333333333334, 0.3333333333333333, 0.25, 0.5]\n",
      "Training with 70%\n",
      "Length of final vocab 192\n",
      "The accuracy for NerualNet is 0.666666666667\n",
      "The accuracy for DecisionTree is 0.333333333333\n",
      "The mean accuracy was 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "csvdata = \"./data/subcategory_2_think_hr_v2.csv\"\n",
    "df = pd.read_csv(csvdata)\n",
    "df = df[:30]\n",
    "catoregories = [\"Compensation\", \"Compliance\", \"Employee Benefits\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(catoregories)\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit([[0],[1],[2]])\n",
    "\n",
    "#df = df.loc[(df['category'] == \"Compensation\") | (df['category'] == \"Compliance\")]\n",
    "#print(df.head(10))\n",
    "\n",
    "\n",
    "ohe_array = ohe.transform(le.transform(df.category).reshape(-1,1)).toarray()\n",
    "df['category'] = le.transform(df.category)\n",
    "df['category'] = list(ohe_array)\n",
    "\n",
    "\n",
    "#vocab = feature_list('./HotWords.csv')\n",
    "try:\n",
    "    vocab_m = load_obj('vocab')\n",
    "except:\n",
    "    vocab_m = build_vocabulary(df.question_class)\n",
    "    save_obj(vocab, 'vocab')\n",
    "\n",
    "avg_acc = []\n",
    "keep_percentages = range(5,11)\n",
    "for keep_percent in keep_percentages:\n",
    "    print(\"Training with {}%\".format(keep_percent*10))\n",
    "    vocab = polarize_dict(df, vocab_m, catoregories, keep_percent)\n",
    "    #print(vocab)\n",
    "\n",
    "\n",
    "    df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "    df = label_features(df, features_master)\n",
    "\n",
    "    acc = train_models(df)\n",
    "    avg_acc.append(acc)\n",
    "    print(\"The mean accuracy was {}\".format(avg_acc))\n",
    "\n",
    "\n",
    "\n",
    "keep_percent = keep_percentages[np.argmax(avg_acc)]\n",
    "\n",
    "\n",
    "print(\"Training with {}%\".format(keep_percent*10))\n",
    "vocab = polarize_dict(df, vocab_m, catoregories, keep_percent)\n",
    "save_obj(vocab, \"final_feature_set\")\n",
    "df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "acc = train_models(df, v=True)\n",
    "\n",
    "print(\"The mean accuracy was {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
