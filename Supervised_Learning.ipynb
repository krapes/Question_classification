{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import csv\n",
    "import pickle\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list(file):\n",
    "    vocab = []\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            else:\n",
    "                print(\"Repeat: {}\".format(word))\n",
    "    return vocab\n",
    "\n",
    "def label_features(df, features_master):\n",
    "        for i,row in df.iterrows():\n",
    "            #message = pruning_dict.remove_nonalphanumeric(row.text)\n",
    "            message = str(row.question_class)\n",
    "            features = build_vocabulary([message], word_drop=False) & features_master\n",
    "            features = features + features_master\n",
    "            features = list(np.array(list(features.values())) - 1)\n",
    "            df.set_value(i,'features',features)\n",
    "        return df\n",
    "\n",
    "def create_feature_dataframe(df, features_master):\n",
    "    return pd.DataFrame(list(df.features), columns=range(len(features_master)))\n",
    "\n",
    "def split_set(x, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(x),\n",
    "                                                        list(y),\n",
    "                                                        test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages, word_drop=True):\n",
    "    cvocabularies = Counter()\n",
    "    #print(\"building vocabulary from {} messages\".format(len(messages)))\n",
    "    for start in range(0, len(messages), 500):\n",
    "        vocabulary = Counter()\n",
    "        for message in messages[start:start + 500]:\n",
    "            message = str(message)\n",
    "            #message = remove_nonalphanumeric(message)\n",
    "            message_split = message.split()\n",
    "            gram_count = 3\n",
    "            grams = []\n",
    "            for i in range(len(message_split) - (gram_count - 1)):\n",
    "                gram = ''\n",
    "                for n in range(gram_count):\n",
    "                    gram = gram + message_split[i+n] + ' '\n",
    "                grams.append(gram[:-1])\n",
    "            vocabulary = vocabulary + Counter(message_split) + Counter(grams)\n",
    "        cvocabularies = cvocabularies + vocabulary\n",
    "    if word_drop == True:\n",
    "        for key, count in dropwhile(\n",
    "                                    lambda key_count: key_count[1] >= (len(messages) * .01),\n",
    "                                    cvocabularies.most_common()):\n",
    "            del cvocabularies[key]\n",
    "    #print(\"The vocabulary contains {} words\".format(len(cvocabularies)))\n",
    "    return cvocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocab(df, vocabulary, group1, group2, percent_saved):\n",
    "\n",
    "    try:\n",
    "        info_v = load_obj(group1)\n",
    "    except:\n",
    "        info = df.groupby('category').get_group(group1)\n",
    "        info_v = build_vocabulary(info.question_class, word_drop=True)\n",
    "        save_obj(info_v, group1)\n",
    "    info_w = info_v.keys()\n",
    "    try:\n",
    "        express_v = load_obj(group2)\n",
    "    except:\n",
    "        express = df.groupby('category').get_group(group2)\n",
    "        express_v = build_vocabulary(express.question_class, word_drop=True)\n",
    "        save_obj(express_v, group2)\n",
    "    express_w = express_v.keys()\n",
    "\n",
    "    common_v = info_v & express_v\n",
    "    common_w = common_v.keys()\n",
    "\n",
    "    words = []\n",
    "    ratios = []\n",
    "\n",
    "    for word in info_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(info_v[word]/float(len(info_v)))\n",
    "            words.append(word)\n",
    "\n",
    "    for word in common_w:\n",
    "        ratios.append((info_v[word]/float(len(info_v))) / (express_v[word]/float(len(express_v))))\n",
    "        words.append(word)\n",
    "\n",
    "    for word in express_w:\n",
    "        if word not in common_w:\n",
    "            ratios.append(express_v[word]/float(len(express_v) * -1))\n",
    "            words.append(word)\n",
    "\n",
    "\n",
    "    threshold = int(len(words) * (percent_saved/2))\n",
    "    top20 = np.argsort(ratios)[-threshold:]\n",
    "    bottom20 = np.argsort(ratios)[:threshold]\n",
    "\n",
    "    polar_words = []\n",
    "    for group in [top20, bottom20]:\n",
    "        for index in group:\n",
    "            polar_words.append(words[index])\n",
    "\n",
    "    vocabulary = Counter(polar_words) & vocabulary\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def polarize_dict(df, vocab_m, catoregories, keep_percent):\n",
    "    pairs = [[0,1], [0,2], [1,2]]\n",
    "    vocabularies = []\n",
    "    for pair in pairs:\n",
    "        a,b = pair\n",
    "        group1 = catoregories[a]\n",
    "        group2 = catoregories[b]\n",
    "        vocab_CC = prune_vocab(df, vocab_m, group1, group2, keep_percent/10.0)\n",
    "        vocabularies.append(vocab_CC)\n",
    "        #print(\"The length of the pruned vocab between {} and {} is {}\".format(group1, group2, len(vocab_CC)))\n",
    "\n",
    "    vocab = vocabularies[0] & vocabularies[1] & vocabularies[2]\n",
    "    print(\"Length of final vocab {}\".format(len(vocab)))\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, v=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_set(df.features, df.category, 0.2)\n",
    "\n",
    "    names = [\"NerualNet\", \"GaussianProcessClassifier\", \n",
    "             \"DecisionTree\", \"OneVsRestClassifier\", \"AdaBoost\",\n",
    "             \"OneVsOneClassifier\", \"OutputCodeClassifier\",  \"KNeighborsClassifier\",\n",
    "             ]\n",
    "    clfs = [MLPClassifier( max_iter=500),\n",
    "            GaussianProcessClassifier(),\n",
    "                DecisionTreeClassifier(),\n",
    "            OneVsRestClassifier( DecisionTreeClassifier()),\n",
    "                AdaBoostClassifier(),\n",
    "           OneVsOneClassifier(DecisionTreeClassifier()),\n",
    "           OutputCodeClassifier(DecisionTreeClassifier()),\n",
    "            KNeighborsClassifier()]\n",
    "    accuracies = []\n",
    "    for clf, name in zip(clfs, names):\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        '''\n",
    "        print(y_test)\n",
    "        print(\"y_predict\")\n",
    "        print(y_predict)\n",
    "        \n",
    "        y_test2 = np.asarray(y_test)\n",
    "        y_test2 = y_test2.dot(ohe.active_features_).astype(int)\n",
    "        y_predict = np.asarray(y_predict.dot(ohe.active_features_).astype(int))\n",
    "        #print(y_test2, y_predict)\n",
    "        '''\n",
    "        score = accuracy_score(y_test, y_predict)\n",
    "        if v == True:\n",
    "            print(\"The accuracy for {} is {}\".format(name, score))\n",
    "        accuracies.append(score)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Training with 80%\n",
      "Length of final vocab 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerri\\Anaconda2\\envs\\thinkhr\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NerualNet is 0.558333333333\n",
      "The accuracy for GaussianProcessClassifier is 0.56\n",
      "The accuracy for DecisionTree is 0.518333333333\n",
      "The accuracy for OneVsRestClassifier is 0.458333333333\n",
      "The accuracy for AdaBoost is 0.55\n",
      "The accuracy for OneVsOneClassifier is 0.531666666667\n",
      "The accuracy for OutputCodeClassifier is 0.498333333333\n",
      "The accuracy for KNeighborsClassifier is 0.503333333333\n",
      "The accuracy for NerualNet is 0.643333333333\n",
      "The accuracy for GaussianProcessClassifier is 0.666666666667\n",
      "The accuracy for DecisionTree is 0.548333333333\n",
      "The accuracy for OneVsRestClassifier is 0.578333333333\n",
      "The accuracy for AdaBoost is 0.656666666667\n",
      "The accuracy for OneVsOneClassifier is 0.598333333333\n",
      "The accuracy for OutputCodeClassifier is 0.551666666667\n",
      "The accuracy for KNeighborsClassifier is 0.508333333333\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-89514b0ec877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mtrain_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m#print(selector_vocab)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0mopt_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselectors_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselectors_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselectors_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mvocab_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "csvdata = \"./data/subcategory_2_think_hr_v2.csv\"\n",
    "df = pd.read_csv(csvdata)\n",
    "df = df[:3000]\n",
    "catoregories = [\"Compensation\", \"Compliance\", \"Employee Benefits\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(catoregories)\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit([[0],[1],[2]])\n",
    "\n",
    "#df = df.loc[(df['category'] == \"Compensation\") | (df['category'] == \"Compliance\")]\n",
    "#print(df.head(10))\n",
    "\n",
    "\n",
    "ohe_array = ohe.transform(le.transform(df.category).reshape(-1,1)).toarray()\n",
    "df['category'] = le.transform(df.category)\n",
    "#df['category'] = list(ohe_array)\n",
    "\n",
    "\n",
    "#vocab = feature_list('./HotWords.csv')\n",
    "try:\n",
    "    vocab_m = load_obj('vocab_master')\n",
    "except:\n",
    "    print(\"Building Vocab\")\n",
    "    vocab_m = build_vocabulary(df.question_class)\n",
    "    save_obj(vocab_m, 'vocab_master')\n",
    "vocab = vocab_m\n",
    "\n",
    "'''\n",
    "avg_acc = []\n",
    "keep_percentages = range(5,11)\n",
    "for keep_percent in keep_percentages:\n",
    "    print(\"Training with {}%\".format(keep_percent*10))\n",
    "    vocab = polarize_dict(df, vocab_m, catoregories, keep_percent)\n",
    "    #print(vocab)\n",
    "\n",
    "\n",
    "    df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "    df = label_features(df, features_master)\n",
    "\n",
    "    acc = train_models(df)\n",
    "    avg_acc.append(acc)\n",
    "    print(\"The mean accuracy was {}\".format(avg_acc))\n",
    "\n",
    "\n",
    "\n",
    "keep_percent = keep_percentages[np.argmax(avg_acc)]\n",
    "'''\n",
    "keep_percent = 8\n",
    "\n",
    "print(\"Training with {}%\".format(keep_percent*10))\n",
    "vocab_p = polarize_dict(df, vocab_m, catoregories, keep_percent)\n",
    "\n",
    "features_master = Counter(list(vocab_p.keys()))\n",
    "\n",
    "#print(features_master)\n",
    "#save_obj(vocab, \"final_feature_set\")\n",
    "df[\"features\"] = [[0] * len(vocab_p)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "train_models(df, v=True)\n",
    "selectors = [chi2]#, f_classif, mutual_info_classif]\n",
    "selectors_vocab = []\n",
    "for selector in selectors:\n",
    "    features = SelectKBest(selector, k=250).fit(list(df.features), list(df.category))\n",
    "    opt_index = features.get_support(indices=True)\n",
    "    selector_vocab = []\n",
    "    for i in opt_index:\n",
    "        selector_vocab.append(list(vocab)[i])\n",
    "    selectors_vocab.append(selector_vocab)\n",
    "    save_obj(selector_vocab, \"final_feature_set\")\n",
    "    features_master = Counter(selector_vocab)\n",
    "    df[\"features\"] = [[0] * len(selector_vocab)] * len(df)\n",
    "    df = label_features(df, features_master)\n",
    "    train_models(df, v=True)\n",
    "    #print(selector_vocab)\n",
    "opt_vocab = Counter(selectors_vocab[0]) & Counter(selectors_vocab[1]) & Counter(selectors_vocab[2]) & vocab_p\n",
    "\n",
    "vocab = list(opt_vocab)\n",
    "features_master = Counter(vocab)\n",
    "df[\"features\"] = [[0] * len(vocab)] * len(df)\n",
    "df = label_features(df, features_master)\n",
    "print(vocab)\n",
    "print(\"Vocabulary lenght: {}\".format(len(vocab)))\n",
    "\n",
    "acc = train_models(df, v=True)\n",
    "\n",
    "#print(\"The mean accuracy was {}\".format(acc))\n",
    "#print(vocab)\n",
    "#print(vocab_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
